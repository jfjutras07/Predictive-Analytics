{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebb6f41",
   "metadata": {
    "papermill": {
     "duration": 0.007354,
     "end_time": "2026-01-08T15:38:26.581498",
     "exception": false,
     "start_time": "2026-01-08T15:38:26.574144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Student Academic Performance Prediction¶\n",
    "## Second Notebook: Data Preparation and Feature Engineering\n",
    "\n",
    "Following the exploratory analysis conducted in the first notebook, the focus of this phase is on preparing the dataset for predictive modeling and constructing informative features that capture the underlying patterns in student performance. Proper data preparation and feature engineering are essential steps to enhance model accuracy, reduce bias, and ensure that the predictive algorithms can effectively leverage both academic and contextual information.\n",
    "\n",
    "This notebook addresses several key aspects of data preparation, including handling missing values, encoding categorical variables, and normalizing numerical features. Additionally, feature engineering techniques are applied to create new variables that may provide greater insight into student performance.\n",
    "\n",
    "This notebook provides a high-level overview of the data preparation and feature engineering process, laying the groundwork for building predictive models that estimate students’ final academic performance.\n",
    "\n",
    "**Author**: J-F Jutras  \n",
    "**Date**: January 2026  \n",
    "**Dataset**: Student Performance — UCI / Kaggle (Portuguese Secondary Education)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71254867",
   "metadata": {
    "papermill": {
     "duration": 0.006135,
     "end_time": "2026-01-08T15:38:26.594631",
     "exception": false,
     "start_time": "2026-01-08T15:38:26.588496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1-Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95e376e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:26.611090Z",
     "iopub.status.busy": "2026-01-08T15:38:26.610718Z",
     "iopub.status.idle": "2026-01-08T15:38:29.903923Z",
     "shell.execute_reply": "2026-01-08T15:38:29.902536Z"
    },
    "papermill": {
     "duration": 3.304368,
     "end_time": "2026-01-08T15:38:29.906250",
     "exception": false,
     "start_time": "2026-01-08T15:38:26.601882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'jfj-utils'...\r\n",
      "remote: Enumerating objects: 2016, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (91/91), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\r\n",
      "remote: Total 2016 (delta 69), reused 21 (delta 21), pack-reused 1925 (from 3)\u001b[K\r\n",
      "Receiving objects: 100% (2016/2016), 664.84 KiB | 14.77 MiB/s, done.\r\n",
      "Resolving deltas: 100% (1301/1301), done.\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "#Download latest version of the dataset\n",
    "path = kagglehub.dataset_download(\"jaimeh1/acamedicperfomance\")\n",
    "\n",
    "#Define dataset path\n",
    "dataset_dir = \"/kaggle/input/acamedicperfomance\"\n",
    "\n",
    "#Load Portuguese dataset\n",
    "port_csv = os.path.join(dataset_dir, \"student_language.csv\")\n",
    "df_port = pd.read_csv(port_csv, sep = \";\")\n",
    "\n",
    "#Load Math dataset\n",
    "math_csv = os.path.join(dataset_dir, \"student_math.csv\")\n",
    "df_math = pd.read_csv(math_csv, sep = \";\")\n",
    "\n",
    "#Clone the public GitHub repository \"jfj-utils\" into the current Kaggle working directory.\n",
    "#This downloads all files and folders from the repo so they can be used in the notebook.\n",
    "!rm -rf /kaggle/working/jfj-utils\n",
    "!git clone https://github.com/jfjutras07/jfj-utils.git\n",
    "\n",
    "#Add the cloned repository to the Python path so Python can import modules from it\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/working/jfj-utils\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c2dac",
   "metadata": {
    "papermill": {
     "duration": 0.006707,
     "end_time": "2026-01-08T15:38:29.920234",
     "exception": false,
     "start_time": "2026-01-08T15:38:29.913527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Column Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b3f75",
   "metadata": {
    "papermill": {
     "duration": 0.007238,
     "end_time": "2026-01-08T15:38:29.934416",
     "exception": false,
     "start_time": "2026-01-08T15:38:29.927178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Variable | Description | Variable | Description |\n",
    "|---------|-------------|---------|-------------|\n",
    "| school | Student's school (GP or MS) | sex | Student's sex (F or M) |\n",
    "| age | Student's age (15–22) | address | Home address type (Urban or Rural) |\n",
    "| famsize | Family size (≤3 or >3) | Pstatus | Parents' cohabitation status |\n",
    "| Medu | Mother's education level (0–4) | Fedu | Father's education level (0–4) |\n",
    "| Mjob | Mother's occupation | Fjob | Father's occupation |\n",
    "| reason | Reason for choosing the school | guardian | Student's guardian |\n",
    "| traveltime | Home-to-school travel time (1–4) | studytime | Weekly study time (1–4) |\n",
    "| failures | Number of past class failures | schoolsup | Extra educational support |\n",
    "| famsup | Family educational support | paid | Extra paid classes (subject-specific) |\n",
    "| activities | Extra-curricular activities | nursery | Attended nursery school |\n",
    "| higher | Intention to pursue higher education | internet | Internet access at home |\n",
    "| romantic | In a romantic relationship | famrel | Family relationship quality (1–5) |\n",
    "| freetime | Free time after school (1–5) | goout | Going out with friends (1–5) |\n",
    "| Dalc | Workday alcohol consumption (1–5) | Walc | Weekend alcohol consumption (1–5) |\n",
    "| health | Current health status (1–5) | absences | Number of school absences |\n",
    "| G1 | First period grade (0–20) | G2 | Second period grade (0–20) |\n",
    "| G3 | Final grade (0–20) |  |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766a4b2",
   "metadata": {
    "papermill": {
     "duration": 0.006661,
     "end_time": "2026-01-08T15:38:29.948936",
     "exception": false,
     "start_time": "2026-01-08T15:38:29.942275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2-Handling Outliers - Absences and Grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe527af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:29.964777Z",
     "iopub.status.busy": "2026-01-08T15:38:29.964401Z",
     "iopub.status.idle": "2026-01-08T15:38:31.660042Z",
     "shell.execute_reply": "2026-01-08T15:38:31.659042Z"
    },
    "papermill": {
     "duration": 1.706954,
     "end_time": "2026-01-08T15:38:31.662438",
     "exception": false,
     "start_time": "2026-01-08T15:38:29.955484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 1, 'absences': 21, 'G1': 16, 'G2': 25, 'G3': 16, 'Total_outliers': 79}\n",
      "{'age': 1, 'absences': 15, 'G1': 0, 'G2': 13, 'G3': 0, 'Total_outliers': 29}\n"
     ]
    }
   ],
   "source": [
    "#Detect outliers in continuous columns of both Portuguese and Math datasets using the IQR method\n",
    "continuous_cols = ['age', 'absences', 'G1', 'G2', 'G3']\n",
    "\n",
    "discrete_cols = [\"Medu\",\"Fedu\",\"traveltime\",\"studytime\",\"failures\",\"famrel\",\"freetime\",\n",
    "                \"goout\",\"Dalc\",\"Walc\",\"health\"]\n",
    "\n",
    "categorical_cols = [\"school\",\"sex\",\"address\",\"famsize\",\"Pstatus\",\"Mjob\",\"Fjob\",\"reason\",\"guardian\",\n",
    "                    \"schoolsup\",\"famsup\",\"paid\",\"activities\",\"nursery\",\"higher\",\"internet\",\"romantic\"]\n",
    "\n",
    "from data_preprocessing.outliers import detect_outliers_iqr\n",
    "print(detect_outliers_iqr(df_port, continuous_cols))\n",
    "print(detect_outliers_iqr(df_math, continuous_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854607e",
   "metadata": {
    "papermill": {
     "duration": 0.006554,
     "end_time": "2026-01-08T15:38:31.675974",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.669420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Extreme values in age are intentionally retained. These observations represent legitimate late-stage student profiles (e.g., 20+ years old) that carry vital predictive signals regarding academic maturity and historical delays.\n",
    "\n",
    "The absences column exhibits significant positive skewness. We apply a logarithmic transformation to these values to neutralize the disproportionate impact of extreme outliers while preserving the ordinal relationship between student attendance and final success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2678e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:31.691417Z",
     "iopub.status.busy": "2026-01-08T15:38:31.690877Z",
     "iopub.status.idle": "2026-01-08T15:38:31.711500Z",
     "shell.execute_reply": "2026-01-08T15:38:31.710422Z"
    },
    "papermill": {
     "duration": 0.031067,
     "end_time": "2026-01-08T15:38:31.713652",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.682585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'absences_log': 0, 'Total_outliers': 0}\n",
      "{'absences_log': 0, 'Total_outliers': 0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Apply log transformation to 'absences' in both datasets\n",
    "df_port['absences_log'] = np.log1p(df_port['absences']) \n",
    "df_math['absences_log'] = np.log1p(df_math['absences'])\n",
    "\n",
    "# nly one representation of the variable is kept in the modeling dataset\n",
    "df_port.drop(columns=['absences'], inplace = True)\n",
    "df_math.drop(columns=['absences'], inplace = True)\n",
    "\n",
    "#Quick check\n",
    "print(detect_outliers_iqr(df_port, ['absences_log']))\n",
    "print(detect_outliers_iqr(df_math, ['absences_log']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803b77f",
   "metadata": {
    "papermill": {
     "duration": 0.006723,
     "end_time": "2026-01-08T15:38:31.727735",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.721012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The raw absences variable was log-transformed to address skewness, and only the transformed version was retained in the modeling dataset to avoid redundancy.\n",
    "\n",
    "Outliers detected in G1, G2, and G3 (specifically zero-inflation) are systematically isolated. By separating these cases for a dedicated descriptive analysis in a later phase, we ensure that our primary regression engines focus on \"active\" academic performance without being biased by dropout events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be232c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:31.743608Z",
     "iopub.status.busy": "2026-01-08T15:38:31.743234Z",
     "iopub.status.idle": "2026-01-08T15:38:31.772811Z",
     "shell.execute_reply": "2026-01-08T15:38:31.771653Z"
    },
    "papermill": {
     "duration": 0.040405,
     "end_time": "2026-01-08T15:38:31.774944",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.734539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Portuguese Analysis ---\n",
      "Zero counts per period:\n",
      "G1     1\n",
      "G2     7\n",
      "G3    15\n",
      "dtype: int64\n",
      "Detailed breakdown:\n",
      "     G1  G2  G3\n",
      "0     0  11  11\n",
      "163  11   9   0\n",
      "440   7   0   0\n",
      "519   8   7   0\n",
      "563   7   0   0\n",
      "567   4   0   0\n",
      "583   8   6   0\n",
      "586   8   8   0\n",
      "597   9   0   0\n",
      "603   5   0   0\n",
      "605   5   0   0\n",
      "610   8   0   0\n",
      "626   7   5   0\n",
      "637   7   7   0\n",
      "639   5   8   0\n",
      "640   7   7   0\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Mathematics Analysis ---\n",
      "Zero counts per period:\n",
      "G1     0\n",
      "G2    13\n",
      "G3    38\n",
      "dtype: int64\n",
      "Detailed breakdown:\n",
      "     G1  G2  G3\n",
      "128   7   4   0\n",
      "130  12   0   0\n",
      "131   8   0   0\n",
      "134   9   0   0\n",
      "135  11   0   0\n",
      "136  10   0   0\n",
      "137   4   0   0\n",
      "140   7   9   0\n",
      "144   5   0   0\n",
      "146   6   7   0\n",
      "148   7   6   0\n",
      "150   6   5   0\n",
      "153   5   0   0\n",
      "160   7   6   0\n",
      "162   7   0   0\n",
      "168   6   7   0\n",
      "170   6   5   0\n",
      "173   8   7   0\n",
      "221   6   5   0\n",
      "239   7   7   0\n",
      "242   6   0   0\n",
      "244   7   0   0\n",
      "259  10   9   0\n",
      "264   9  10   0\n",
      "269   6   0   0\n",
      "296  10   9   0\n",
      "310   9   9   0\n",
      "316   8   8   0\n",
      "332   7   0   0\n",
      "333   8   8   0\n",
      "334  10   9   0\n",
      "337   7   8   0\n",
      "341  10  10   0\n",
      "343   9   8   0\n",
      "367   7   6   0\n",
      "383   6   5   0\n",
      "387   7   5   0\n",
      "389   6   5   0\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_zero_grades(df, dataset_name):\n",
    "    target_grades = ['G1', 'G2', 'G3']\n",
    "    \n",
    "    #Count occurrences per column\n",
    "    zero_counts = (df[target_grades] == 0).sum()\n",
    "    \n",
    "    #Filter students with at least one zero\n",
    "    zero_anywhere = df[(df['G1'] == 0) | (df['G2'] == 0) | (df['G3'] == 0)][['G1', 'G2', 'G3']]\n",
    "    \n",
    "    #Identify Sudden Dropouts (Passing G1/G2, but 0 in G3)\n",
    "    sudden_dropouts = zero_anywhere[(zero_anywhere['G1'] > 0) & (zero_anywhere['G2'] > 0) & (zero_anywhere['G3'] == 0)]\n",
    "    \n",
    "    #Identify the Recovery Case (0 in G1, but > 0 in G3)\n",
    "    recovery_cases = zero_anywhere[(zero_anywhere['G1'] == 0) & (zero_anywhere['G3'] > 0)]\n",
    "    \n",
    "    print(f\"--- {dataset_name} Analysis ---\")\n",
    "    print(f\"Zero counts per period:\\n{zero_counts}\")\n",
    "    print(\"Detailed breakdown:\")\n",
    "    print(zero_anywhere)\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "#Run for both datasets\n",
    "analyze_zero_grades(df_port, \"Portuguese\")\n",
    "analyze_zero_grades(df_math, \"Mathematics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cfacc6",
   "metadata": {
    "papermill": {
     "duration": 0.006765,
     "end_time": "2026-01-08T15:38:31.788755",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.781990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The 53 students (15 in Portuguese and 38 in Mathematics) who finished with a final grade of 0 are removed from this stage. A final zero is usually an \"administrative\" event (dropping out, long-term absence) rather than a reflection of actual academic skill. Keeping these cases would compromise the regression models by forcing them to predict a total collapse that follows a different logic than regular grading. These observations are moved to Notebook 4 for a specific descriptive study.\n",
    "\n",
    "The student in the Portuguese dataset who started with a 0 in G1 but improved to an 11 in G3 is intentionally retained. This case represents a real academic success story and provides the model with a crucial signal for resilience, demonstrating that an initial failure is not an inescapable outcome.\n",
    "\n",
    "By separating \"dropouts\" from \"active students,\" the models become significantly more accurate at predicting the actual grades of the population remaining within the school system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b1e9c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:31.806528Z",
     "iopub.status.busy": "2026-01-08T15:38:31.806038Z",
     "iopub.status.idle": "2026-01-08T15:38:31.823460Z",
     "shell.execute_reply": "2026-01-08T15:38:31.821702Z"
    },
    "papermill": {
     "duration": 0.030209,
     "end_time": "2026-01-08T15:38:31.825758",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.795549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction successful:\n",
      "-> df_all_dropouts: 53 rows\n",
      "-> Cleaned df_port: 634 rows\n",
      "-> Cleaned df_math: 357 rows\n"
     ]
    }
   ],
   "source": [
    "#Identify and isolate dropouts (G3 = 0)\n",
    "#We create temporary copies and add a 'subject' column to track origin after merging\n",
    "port_dropouts = df_port[df_port['G3'] == 0].copy()\n",
    "port_dropouts['subject'] = 'Portuguese'\n",
    "\n",
    "math_dropouts = df_math[df_math['G3'] == 0].copy()\n",
    "math_dropouts['subject'] = 'Mathematics'\n",
    "\n",
    "#Create the combined dropout dataset for further analysis in Notebook 4\n",
    "#Merging the 53 cases (15 Port + 38 Math) into a single dataframe\n",
    "df_all_dropouts = pd.concat([port_dropouts, math_dropouts], axis=0).reset_index(drop=True)\n",
    "\n",
    "#Clean original datasets\n",
    "df_port = df_port[df_port['G3'] > 0].copy()\n",
    "df_math = df_math[df_math['G3'] > 0].copy()\n",
    "\n",
    "#Verification ---\n",
    "print(f\"Extraction successful:\")\n",
    "print(f\"-> df_all_dropouts: {df_all_dropouts.shape[0]} rows\")\n",
    "print(f\"-> Cleaned df_port: {df_port.shape[0]} rows\")\n",
    "print(f\"-> Cleaned df_math: {df_math.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1eeee",
   "metadata": {
    "papermill": {
     "duration": 0.007158,
     "end_time": "2026-01-08T15:38:31.839983",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.832825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3-Feature Engineering Design and Variable Transformation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f07f86",
   "metadata": {
    "papermill": {
     "duration": 0.008201,
     "end_time": "2026-01-08T15:38:31.856748",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.848547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Demographics\n",
    "\n",
    "| Variable | Transformation / Option | Type | Justification | Priority / Scenario |\n",
    "|----------|------------------------|------|---------------|------------------|\n",
    "| Age | Keep numeric | Continuous | Weak direct correlation with grades, captures older repeaters | Standard |\n",
    "| Sex | Binary encoding (0=male,1=female) | Binary | Stable effect across grades | Priority |\n",
    "| School (GP/MS) | Binary (GP=1, MS=0) | Binary | Moderate, consistent effect | Priority |\n",
    "| Address (Urban/Rural) | Binary (Urban=1, Rural=0) | Binary | Weak effect, stable | Standard |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead8008",
   "metadata": {
    "papermill": {
     "duration": 0.006797,
     "end_time": "2026-01-08T15:38:31.870832",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.864035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Family Structure and Socio-Economic Background\n",
    "\n",
    "| Variable | Transformation / Option | Type | Justification | Priority / Scenario |\n",
    "|----------|------------------------|------|---------------|------------------|\n",
    "| Famsize | Binary (GT3 / LE3) | Binary | Family structure control variable; weak but standard contextual effect | Standard |\n",
    "| Pstatus | Binary (T = 1, A = 0) | Binary | Minor but stable association; captures household stability | Standard |\n",
    "| Medu | Keep ordinal (0–4) | Ordinal | Positive, stable effect on academic performance | Priority |\n",
    "| Fedu | Keep ordinal (0–4) | Ordinal | Positive, stable effect on academic performance | Priority |\n",
    "| Mjob | One-hot encoding | Categorical | Occupational context not fully captured by education | Standard |\n",
    "| Fjob | One-hot encoding | Categorical | Complementary socio-economic signal | Standard |\n",
    "| Guardian | One-hot (mother / father / other) | Categorical | Weak/modest contextual effect | Standard |\n",
    "| Parental Education Level | Composite: mean(Medu, Fedu) | Continuous composite | Captures parental educational capital while reducing multicollinearity | Priority (alternative core) |\n",
    "| Parental Education Level (binned) | Low (0–1) / Medium (2) / High (3–4) | Ordinal | Could model socio-educational strata and potential non-linear effects | Considered |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818be74c",
   "metadata": {
    "papermill": {
     "duration": 0.007306,
     "end_time": "2026-01-08T15:38:31.885675",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.878369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### School Context and Academic Support\n",
    "\n",
    "| Variable | Transformation / Option | Type | Justification | Priority / Scenario |\n",
    "|----------|------------------------|------|---------------|------------------|\n",
    "| Reason | One-hot | Categorical | Weak effect, useful for interactions | Priority |\n",
    "| Traveltime | Keep numeric | Continuous | Slight effect on grades | Standard |\n",
    "| Schoolsup | Binary (Yes=1, No=0) | Binary | Indicates prior academic difficulty | Standard |\n",
    "| Famsup | Binary (Yes=1, No=0) | Binary | Slight positive effect, stable | Standard |\n",
    "| Paid | Binary (Yes=1, No=0) | Binary | Weak effect, may reflect extra support | Standard |\n",
    "| Nursery | Binary (Yes=1, No=0) | Binary | Stable, minor effect | Standard |\n",
    "| Higher | Binary (Yes=1, No=0) | Binary | Strong positive effect, major predictor | Priority |\n",
    "| Internet | Binary (Yes=1, No=0) | Binary | Minor effect, optional for interactions | Standard |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d45b5",
   "metadata": {
    "papermill": {
     "duration": 0.006687,
     "end_time": "2026-01-08T15:38:31.899613",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.892926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Student Behavior and Lifestyle\n",
    "\n",
    "| Variable | Transformation / Option | Type | Justification | Priority / Scenario |\n",
    "|----------|------------------------|------|---------------|------------------|\n",
    "| Studytime | Keep numeric | Ordinal | Slight positive effect on grades | Standard |\n",
    "| Failures | Keep numeric | Continuous | Strong negative effect on G3 | Priority |\n",
    "| Failures | Bin: 0 / 1 / ≥2 | Ordinal | Could capture non-linear impact on grades | Considered |\n",
    "| Activities | Binary (Yes=1, No=0) | Binary | Small effect | Standard |\n",
    "| Freetime | Keep numeric | Ordinal | Minor effect, stable | Standard |\n",
    "| Goout | Keep numeric | Ordinal | Modest negative effect | Standard |\n",
    "| Romantic | Binary (Yes=1, No=0) | Binary | Slight negative effect | Standard |\n",
    "| Absences | Keep numeric (already log-transformed) | Continuous | Weak/modest effect | Standard |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3aaba",
   "metadata": {
    "papermill": {
     "duration": 0.007012,
     "end_time": "2026-01-08T15:38:31.913555",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.906543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Health and Well-being\n",
    "\n",
    "| Variable | Transformation / Option | Type | Justification | Priority / Scenario |\n",
    "|----------|------------------------|------|---------------|------------------|\n",
    "| Health | Keep numeric | Ordinal | Minor effect, stable across grades | Standard |\n",
    "| Famrel | Keep numeric | Ordinal | Moderate positive, stable family effect | Standard |\n",
    "| Dalc (daily alcohol) | Keep numeric | Ordinal | Slight negative effect, behavior-related | Standard |\n",
    "| Walc (weekend alcohol) | Keep numeric | Ordinal | Slight negative effect, correlated with Dalc | Standard |\n",
    "| Alcohol Consumption Index | Composite (Dalc + Walc) | Composite numeric | Captures latent alcohol behavior, reduces redundancy, more stable than individual effects | Sensitivity scenario |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739ca6e",
   "metadata": {
    "papermill": {
     "duration": 0.006847,
     "end_time": "2026-01-08T15:38:31.928038",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.921191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Academic Performance\n",
    "\n",
    "| Variable | Transformation / Option | Type | Justification | Priority / Scenario |\n",
    "|----------|------------------------|------|---------------|------------------|\n",
    "| G1 | Keep numeric | Continuous | Early academic level; component of overall achievement | Standard |\n",
    "| G2 | Keep numeric | Continuous | Mid-year academic level; strong contributor to overall achievement | Standard |\n",
    "| Prior Grades Mean | (G1 + G2) / 2 | Continuous composite | Could capture prior academic level before final evaluation as an alternative baseline | Considered |\n",
    "| Early Progress Index     | G2 − G1                        | Continuous        | Captures early learning dynamics (improvement or decline)           | Sensitivity scenario              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba39480",
   "metadata": {
    "papermill": {
     "duration": 0.006696,
     "end_time": "2026-01-08T15:38:31.942137",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.935441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4-Data Partitioning for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9849d61b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:31.957765Z",
     "iopub.status.busy": "2026-01-08T15:38:31.957436Z",
     "iopub.status.idle": "2026-01-08T15:38:32.390335Z",
     "shell.execute_reply": "2026-01-08T15:38:32.388987Z"
    },
    "papermill": {
     "duration": 0.443421,
     "end_time": "2026-01-08T15:38:32.392478",
     "exception": false,
     "start_time": "2026-01-08T15:38:31.949057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese dataset: train=(507, 33), test=(127, 33)\n",
      "Math dataset: train=(285, 33), test=(72, 33)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Define the target variable\n",
    "target_col = 'G3'\n",
    "\n",
    "#Portuguese dataset\n",
    "# Split into train and test sets (80/20) while keeping distribution of target\n",
    "train_port, test_port = train_test_split(\n",
    "    df_port, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42, \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "#Quick check of sizes\n",
    "print(f\"Portuguese dataset: train={train_port.shape}, test={test_port.shape}\")\n",
    "\n",
    "#Mathematics dataset\n",
    "train_math, test_math = train_test_split(\n",
    "    df_math,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "print(f\"Math dataset: train={train_math.shape}, test={test_math.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3a9b8",
   "metadata": {
    "papermill": {
     "duration": 0.006849,
     "end_time": "2026-01-08T15:38:32.406388",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.399539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.5-Baseline Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f7a61c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.422216Z",
     "iopub.status.busy": "2026-01-08T15:38:32.421637Z",
     "iopub.status.idle": "2026-01-08T15:38:32.485977Z",
     "shell.execute_reply": "2026-01-08T15:38:32.484789Z"
    },
    "papermill": {
     "duration": 0.074846,
     "end_time": "2026-01-08T15:38:32.488263",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.413417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary encoding successfully applied to 13 columns on 2 dataset(s).\n",
      "Binary encoding successfully applied to 13 columns on 2 dataset(s).\n"
     ]
    }
   ],
   "source": [
    "#Define binary_mappings for binary encoding\n",
    "binary_mappings = {\n",
    "    'sex': {'F': 1, 'M': 0},\n",
    "    'school': {'GP': 1, 'MS': 0},\n",
    "    'address': {'U': 1, 'R': 0},\n",
    "    'famsize': {'GT3': 1, 'LE3': 0},\n",
    "    'Pstatus': {'T': 1, 'A': 0},\n",
    "    'schoolsup': {'yes': 1, 'no': 0},\n",
    "    'famsup': {'yes': 1, 'no': 0},\n",
    "    'paid': {'yes': 1, 'no': 0},\n",
    "    'nursery': {'yes': 1, 'no': 0},\n",
    "    'higher': {'yes': 1, 'no': 0},\n",
    "    'activities': {'yes': 1, 'no': 0},\n",
    "    'romantic': {'yes': 1, 'no': 0},\n",
    "    'internet': {'yes': 1, 'no': 0}\n",
    "}\n",
    "\n",
    "from data_preprocessing.encoding import binary_encode_columns\n",
    "\n",
    "#Apply binary encoding on train sets first\n",
    "train_port, train_math = binary_encode_columns(\n",
    "    dfs=[train_port, train_math],\n",
    "    binary_mappings=binary_mappings,\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "#Apply the same encoding to the test sets\n",
    "test_port, test_math = binary_encode_columns(\n",
    "    dfs=[test_port, test_math],\n",
    "    binary_mappings=binary_mappings,\n",
    "    strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a40550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.504288Z",
     "iopub.status.busy": "2026-01-08T15:38:32.503918Z",
     "iopub.status.idle": "2026-01-08T15:38:32.544559Z",
     "shell.execute_reply": "2026-01-08T15:38:32.543168Z"
    },
    "papermill": {
     "duration": 0.051189,
     "end_time": "2026-01-08T15:38:32.546712",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.495523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding successfully applied to 4 columns on 2 dataset(s).\n"
     ]
    }
   ],
   "source": [
    "#Columns to one-hot encode (categorical features with multiple categories)\n",
    "parent_job_cols = ['Mjob', 'Fjob', 'guardian', 'reason']\n",
    "\n",
    "from data_preprocessing.encoding import one_hot_encode_columns\n",
    "\n",
    "#Apply one-hot encoding on train sets\n",
    "train_port, train_math = one_hot_encode_columns(\n",
    "    dfs=[train_port, train_math],\n",
    "    categorical_cols=parent_job_cols,\n",
    "    drop_first=False \n",
    ")\n",
    "\n",
    "#After one-hot encoding, the train and test sets may have different columns because some categories\n",
    "#may only appear in train or test. We must ensure the test set has the same columns as train set\n",
    "#to avoid errors during scaling or model training.\n",
    "def align_columns(train_df, test_df):\n",
    "    #Add missing columns in test set with 0 (the category did not appear in test)\n",
    "    for col in train_df.columns:\n",
    "        if col not in test_df.columns:\n",
    "            test_df[col] = 0\n",
    "    #Keep only the columns that exist in train set (remove extra unseen categories in test)\n",
    "    test_df = test_df[train_df.columns]\n",
    "    return test_df\n",
    "\n",
    "#This guarantees that both train and test datasets have identical column names and order.\n",
    "#Without this step, applying normalization or feeding data to models would raise errors.\n",
    "test_port = align_columns(train_port, test_port)\n",
    "test_math = align_columns(train_math, test_math)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23687167",
   "metadata": {
    "papermill": {
     "duration": 0.008249,
     "end_time": "2026-01-08T15:38:32.562294",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.554045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.6-Sensitivity Features - Composite and Alternative Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a06818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.580077Z",
     "iopub.status.busy": "2026-01-08T15:38:32.579272Z",
     "iopub.status.idle": "2026-01-08T15:38:32.594859Z",
     "shell.execute_reply": "2026-01-08T15:38:32.593460Z"
    },
    "papermill": {
     "duration": 0.027875,
     "end_time": "2026-01-08T15:38:32.597838",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.569963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Parental Education Level (mean)\n",
    "train_port['Parental_Edu_Level'] = train_port[['Medu','Fedu']].mean(axis=1)\n",
    "train_math['Parental_Edu_Level'] = train_math[['Medu','Fedu']].mean(axis=1)\n",
    "\n",
    "#Test sets (use same columns)\n",
    "test_port['Parental_Edu_Level'] = test_port[['Medu','Fedu']].mean(axis=1)\n",
    "test_math['Parental_Edu_Level'] = test_math[['Medu','Fedu']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3107e5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.614149Z",
     "iopub.status.busy": "2026-01-08T15:38:32.613347Z",
     "iopub.status.idle": "2026-01-08T15:38:32.649601Z",
     "shell.execute_reply": "2026-01-08T15:38:32.648232Z"
    },
    "papermill": {
     "duration": 0.047216,
     "end_time": "2026-01-08T15:38:32.652249",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.605033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding successfully applied to 2 columns on 2 dataset(s).\n",
      "Label encoding successfully applied to 2 columns on 2 dataset(s).\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing.encoding import label_encode_columns\n",
    "\n",
    "#Parental Education Level (binned)\n",
    "edu_bins = [-0.1, 1, 2, 4]          \n",
    "edu_labels = ['Low', 'Medium', 'High']\n",
    "\n",
    "for df in [train_port, train_math]:\n",
    "    df['Parental_Edu_Bin'] = pd.cut(\n",
    "        df['Parental_Edu_Level'],\n",
    "        bins=edu_bins,\n",
    "        labels=edu_labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "for df in [test_port, test_math]:\n",
    "    df['Parental_Edu_Bin'] = pd.cut(\n",
    "        df['Parental_Edu_Level'],\n",
    "        bins=edu_bins,\n",
    "        labels=edu_labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "#Failures (binned)\n",
    "fail_bins = [-0.1, 0, 1, np.inf]    \n",
    "fail_labels = ['0', '1', '2']       \n",
    "\n",
    "for df in [train_port, train_math]:\n",
    "    df['Failures_Bin'] = pd.cut(\n",
    "        df['failures'],\n",
    "        bins=fail_bins,\n",
    "        labels=fail_labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "for df in [test_port, test_math]:\n",
    "    df['Failures_Bin'] = pd.cut(\n",
    "        df['failures'],\n",
    "        bins=fail_bins,\n",
    "        labels=fail_labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "#Label encoding\n",
    "bins_cols = [\"Parental_Edu_Bin\", \"Failures_Bin\"]\n",
    "train_port, train_math = label_encode_columns([train_port, train_math], bins_cols)\n",
    "test_port, test_math = label_encode_columns([test_port, test_math], bins_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c081a0e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.668688Z",
     "iopub.status.busy": "2026-01-08T15:38:32.668295Z",
     "iopub.status.idle": "2026-01-08T15:38:32.676386Z",
     "shell.execute_reply": "2026-01-08T15:38:32.675412Z"
    },
    "papermill": {
     "duration": 0.019036,
     "end_time": "2026-01-08T15:38:32.678616",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.659580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Alcohol Consumption Index\n",
    "train_port['Alcohol_Index'] = train_port['Dalc'] + train_port['Walc']\n",
    "train_math['Alcohol_Index'] = train_math['Dalc'] + train_math['Walc']\n",
    "\n",
    "test_port['Alcohol_Index'] = test_port['Dalc'] + test_port['Walc']\n",
    "test_math['Alcohol_Index'] = test_math['Dalc'] + test_math['Walc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736b097e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.695429Z",
     "iopub.status.busy": "2026-01-08T15:38:32.695055Z",
     "iopub.status.idle": "2026-01-08T15:38:32.704485Z",
     "shell.execute_reply": "2026-01-08T15:38:32.703308Z"
    },
    "papermill": {
     "duration": 0.02073,
     "end_time": "2026-01-08T15:38:32.706687",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.685957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prior Grades Mean\n",
    "train_port['Prior_Grades_Mean'] = (train_port['G1'] + train_port['G2']) / 2\n",
    "train_math['Prior_Grades_Mean'] = (train_math['G1'] + train_math['G2']) / 2\n",
    "\n",
    "test_port['Prior_Grades_Mean'] = (test_port['G1'] + test_port['G2']) / 2\n",
    "test_math['Prior_Grades_Mean'] = (test_math['G1'] + test_math['G2']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4100db34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.725232Z",
     "iopub.status.busy": "2026-01-08T15:38:32.724248Z",
     "iopub.status.idle": "2026-01-08T15:38:32.732044Z",
     "shell.execute_reply": "2026-01-08T15:38:32.731009Z"
    },
    "papermill": {
     "duration": 0.019316,
     "end_time": "2026-01-08T15:38:32.734156",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.714840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Early Progress Index\n",
    "train_port['Early_Progress_Index'] = train_port['G2'] - train_port['G1']\n",
    "train_math['Early_Progress_Index'] = train_math['G2'] - train_math['G1']\n",
    "\n",
    "test_port['Early_Progress_Index'] = test_port['G2'] - test_port['G1']\n",
    "test_math['Early_Progress_Index'] = test_math['G2'] - test_math['G1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7ffb4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.751685Z",
     "iopub.status.busy": "2026-01-08T15:38:32.750291Z",
     "iopub.status.idle": "2026-01-08T15:38:32.771246Z",
     "shell.execute_reply": "2026-01-08T15:38:32.770232Z"
    },
    "papermill": {
     "duration": 0.03181,
     "end_time": "2026-01-08T15:38:32.773331",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.741521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Parental_Edu_Level  Parental_Edu_Bin  Failures_Bin  Alcohol_Index  \\\n",
      "489                 1.0                 0             1              7   \n",
      "422                 2.5                 2             0              3   \n",
      "104                 3.5                 2             0              2   \n",
      "114                 1.5                 1             0              2   \n",
      "350                 1.5                 1             1              2   \n",
      "\n",
      "     Prior_Grades_Mean  Early_Progress_Index  \n",
      "489                8.5                     1  \n",
      "422               11.5                     3  \n",
      "104               16.0                     0  \n",
      "114                9.5                    -1  \n",
      "350                9.5                     1  \n",
      "     Parental_Edu_Level  Parental_Edu_Bin  Failures_Bin  Alcohol_Index  \\\n",
      "308                 3.0                 2             1              3   \n",
      "368                 2.5                 2             0              3   \n",
      "315                 2.5                 2             1              2   \n",
      "75                  3.5                 2             0              5   \n",
      "280                 2.5                 2             0              6   \n",
      "\n",
      "     Prior_Grades_Mean  Early_Progress_Index  \n",
      "308               13.5                    -3  \n",
      "368               10.5                    -1  \n",
      "315               12.0                    -2  \n",
      "75                 9.0                     0  \n",
      "280                8.0                     0  \n",
      "     Parental_Edu_Level  Parental_Edu_Bin  Failures_Bin  Alcohol_Index  \\\n",
      "397                 2.0                 1             0              3   \n",
      "249                 2.5                 2             0              4   \n",
      "216                 1.0                 0             0              4   \n",
      "354                 1.5                 1             0              3   \n",
      "551                 2.5                 2             0              4   \n",
      "\n",
      "     Prior_Grades_Mean  Early_Progress_Index  \n",
      "397               11.0                     2  \n",
      "249               12.0                     0  \n",
      "216               14.5                     1  \n",
      "354               12.0                     0  \n",
      "551               13.5                     1  \n",
      "     Parental_Edu_Level  Parental_Edu_Bin  Failures_Bin  Alcohol_Index  \\\n",
      "243                 4.0                 2             0              3   \n",
      "42                  4.0                 2             0              2   \n",
      "319                 4.0                 2             0              6   \n",
      "328                 4.0                 2             0              4   \n",
      "56                  3.5                 2             0              2   \n",
      "\n",
      "     Prior_Grades_Mean  Early_Progress_Index  \n",
      "243               12.5                    -1  \n",
      "42                18.5                    -1  \n",
      "319               11.0                     0  \n",
      "328                9.5                    -1  \n",
      "56                14.5                     1  \n"
     ]
    }
   ],
   "source": [
    "#Quick check on train sets\n",
    "sensitivity_cols = [\n",
    "    'Parental_Edu_Level', 'Parental_Edu_Bin', 'Failures_Bin',\n",
    "    'Alcohol_Index', 'Prior_Grades_Mean', 'Early_Progress_Index'\n",
    "]\n",
    "\n",
    "print(train_port[sensitivity_cols].head(5))\n",
    "print(train_math[sensitivity_cols].head(5))\n",
    "print(test_port[sensitivity_cols].head(5))\n",
    "print(test_math[sensitivity_cols].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c091686",
   "metadata": {
    "papermill": {
     "duration": 0.00866,
     "end_time": "2026-01-08T15:38:32.789403",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.780743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.7-Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4edd64a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.808566Z",
     "iopub.status.busy": "2026-01-08T15:38:32.808237Z",
     "iopub.status.idle": "2026-01-08T15:38:32.847079Z",
     "shell.execute_reply": "2026-01-08T15:38:32.845907Z"
    },
    "papermill": {
     "duration": 0.050934,
     "end_time": "2026-01-08T15:38:32.849087",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.798153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete for portuguese. No warnings.\n",
      "Normalization complete for math. No warnings.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Datasets dictionary\n",
    "datasets = {\n",
    "    \"portuguese\": {\"train\": train_port, \"test\": test_port},\n",
    "    \"math\": {\"train\": train_math, \"test\": test_math}\n",
    "}\n",
    "\n",
    "#Loop over datasets\n",
    "normalized_datasets = {}\n",
    "\n",
    "for name, dfs in datasets.items():\n",
    "    train_df = dfs[\"train\"]\n",
    "    test_df = dfs[\"test\"]\n",
    "    \n",
    "    #Identify numeric columns (including MCA components)\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    #Remove target G3 from scaling to keep it on its original 0-20 scale\n",
    "    if 'G3' in numeric_cols:\n",
    "        numeric_cols.remove('G3')\n",
    "    \n",
    "    #Remove constant columns (std = 0) to avoid division by zero errors\n",
    "    numeric_cols = [c for c in numeric_cols if train_df[c].std() > 1e-10]\n",
    "    \n",
    "    non_numeric_cols = train_df.columns.difference(numeric_cols)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #Scale train numeric columns\n",
    "    train_norm_num = pd.DataFrame(\n",
    "        scaler.fit_transform(train_df[numeric_cols]),\n",
    "        columns=numeric_cols,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    #Scale test numeric columns using the fitted scaler\n",
    "    test_norm_num = pd.DataFrame(\n",
    "        scaler.transform(test_df[numeric_cols]),\n",
    "        columns=numeric_cols,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    #Reconstruct datasets by merging scaled and non-scaled columns\n",
    "    normalized_datasets[name] = {\n",
    "        \"train\": pd.concat([train_norm_num, train_df[non_numeric_cols]], axis=1),\n",
    "        \"test\": pd.concat([test_norm_num, test_df[non_numeric_cols]], axis=1),\n",
    "        \"scaler\": scaler,\n",
    "        \"numeric_cols\": numeric_cols\n",
    "    }\n",
    "    \n",
    "    #Print inside the loop to see progress for each dataset\n",
    "    print(f\"Normalization complete for {name}. No warnings.\")\n",
    "\n",
    "#Update the original variables with normalized data\n",
    "train_port = normalized_datasets[\"portuguese\"][\"train\"]\n",
    "test_port = normalized_datasets[\"portuguese\"][\"test\"]\n",
    "train_math = normalized_datasets[\"math\"][\"train\"]\n",
    "test_math = normalized_datasets[\"math\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b43f6",
   "metadata": {
    "papermill": {
     "duration": 0.007254,
     "end_time": "2026-01-08T15:38:32.864352",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.857098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To accommodate a wide range of potential architectures, all numeric and ordinal columns were normalized (except our target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69244d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:32.880998Z",
     "iopub.status.busy": "2026-01-08T15:38:32.880617Z",
     "iopub.status.idle": "2026-01-08T15:38:33.044065Z",
     "shell.execute_reply": "2026-01-08T15:38:33.042908Z"
    },
    "papermill": {
     "duration": 0.174422,
     "end_time": "2026-01-08T15:38:33.046321",
     "exception": false,
     "start_time": "2026-01-08T15:38:32.871899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Missing values\n",
      "No missing values detected.\n",
      "\n",
      "#Feature types\n",
      "No non-numeric columns detected.\n",
      "\n",
      "#Feature variance\n",
      "No constant columns detected.\n",
      "\n",
      "#Outliers\n",
      "##Original continuous features\n",
      "- Total outliers detected: 30\n",
      "- Top contributing features:\n",
      "  1. G2: 14\n",
      "  2. G1: 10\n",
      "  3. Early_Progress_Index: 5\n",
      "  4. G3: 1\n",
      "##PCA/MCA-derived features\n",
      "No significant PCA/MCA-related outliers detected.\n",
      "\n",
      "#High correlations (|r| ≥ 0.70)\n",
      "- Strongest correlations:\n",
      "  1. failures ↔ Failures_Bin: 0.980\n",
      "  2. G1 ↔ Prior_Grades_Mean: 0.971\n",
      "  3. G2 ↔ Prior_Grades_Mean: 0.969\n",
      "  4. Walc ↔ Alcohol_Index: 0.935\n",
      "  5. G2 ↔ G3: 0.931\n",
      "  6. Prior_Grades_Mean ↔ G3: 0.926\n",
      "  7. Medu ↔ Parental_Edu_Level: 0.918\n",
      "  8. Fedu ↔ Parental_Edu_Level: 0.909\n",
      "  9. Parental_Edu_Level ↔ Parental_Edu_Bin: 0.899\n",
      "  10. G1 ↔ G2: 0.880\n",
      "- 6 additional correlated pairs above threshold\n",
      "\n",
      "#Target validation\n",
      "No target specified.\n",
      "\n",
      "#Dataset size\n",
      "No size-related risks detected.\n",
      "\n",
      "#Final assessment\n",
      "Dataset is usable for regression, but issues above should be reviewed depending on model choice (linear, regularized, tree-based, or neural).\n"
     ]
    }
   ],
   "source": [
    "#Check dataset quality (train Portuguese example)\n",
    "from eda.premodeling_check import premodeling_regression_check\n",
    "print(premodeling_regression_check(train_port))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d88ddf",
   "metadata": {
    "papermill": {
     "duration": 0.007382,
     "end_time": "2026-01-08T15:38:33.061332",
     "exception": false,
     "start_time": "2026-01-08T15:38:33.053950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The diagnostic report confirms that the dataset is clean and structurally sound, with no missing values or constant features. The identified high correlations and outliers are not data flaws but rather high-density signals captured during feature engineering. \n",
    "\n",
    "These findings justify our sensitivity analysis strategy: by testing different feature subsets, we can leverage this information while neutralizing multicollinearity. The dataset is fully validated and ready for robust predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f4d9e",
   "metadata": {
    "papermill": {
     "duration": 0.007565,
     "end_time": "2026-01-08T15:38:33.076943",
     "exception": false,
     "start_time": "2026-01-08T15:38:33.069378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.9-Summary - Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5c9e9",
   "metadata": {
    "papermill": {
     "duration": 0.007962,
     "end_time": "2026-01-08T15:38:33.092837",
     "exception": false,
     "start_time": "2026-01-08T15:38:33.084875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "| Category | Main Transformation | Notes / Insights |\n",
    "| :--- | :--- | :--- |\n",
    "| Outlier Management | Log-transformation of Absences | Raw absences showed high skewness. Applying log(1+x) successfully neutralized extreme values while preserving the distribution, eliminating the need to delete data. |\n",
    "| Data Partitioning | 80/20 Train-Test Split | Stratified-like split (random_state=42) used to ensure target distribution is preserved across sets before any feature engineering to prevent leakage. |\n",
    "| Categorical Encoding | Binary & One-Hot Encoding | 13 binary features (sex, school, etc.) mapped to 0/1. Multi-category features (Mjob, Fjob) expanded via One-Hot. align_columns logic ensures consistency between Train and Test. |\n",
    "| Parental Education | Composite Index & Binning | Created Parental_Edu_Level (mean) and Parental_Edu_Bin. Reduces Medu/Fedu redundancy while capturing the household's total educational capital. |\n",
    "| Academic Momentum | Early Progress & Prior Mean | Engineered Early_Progress_Index (G2-G1) and Prior_Grades_Mean. These capture the \"velocity\" of student improvement, often more predictive than static grades. |\n",
    "| Behavioral Indexes | Alcohol Index & Failures Bin | Combined Dalc and Walc into a single Alcohol_Index to reduce noise. Binned failures into 0, 1, and 2+ to capture non-linear impacts on performance. |\n",
    "| Feature Scaling | Standardization (Z-score) | All numeric and ordinal features normalized using StandardScaler. Fit on Train only to prevent Data Leakage; essential for the convergence of Neural Networks. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dd84e",
   "metadata": {
    "papermill": {
     "duration": 0.007349,
     "end_time": "2026-01-08T15:38:33.108225",
     "exception": false,
     "start_time": "2026-01-08T15:38:33.100876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.10-Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c62301cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T15:38:33.124880Z",
     "iopub.status.busy": "2026-01-08T15:38:33.124497Z",
     "iopub.status.idle": "2026-01-08T15:38:33.134856Z",
     "shell.execute_reply": "2026-01-08T15:38:33.133773Z"
    },
    "papermill": {
     "duration": 0.021155,
     "end_time": "2026-01-08T15:38:33.136848",
     "exception": false,
     "start_time": "2026-01-08T15:38:33.115693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Export Successful: student_performance_sensitivity_bundle.pkl ---\n",
      "Metadata: 'features_base' and 'features_sensitivity' lists included.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# On récupère les listes de variables que TU as définies plus haut\n",
    "# Pour reconstruire la liste des prédicteurs de base (sans G1, G2, G3)\n",
    "base_features = (\n",
    "    [c for c in continuous_cols if c not in ['G1', 'G2', 'G3']] + \n",
    "    discrete_cols + \n",
    "    categorical_cols + \n",
    "    ['absences_log']\n",
    ")\n",
    "\n",
    "# Output filename\n",
    "export_filename = \"student_performance_sensitivity_bundle.pkl\"\n",
    "\n",
    "# Bundle dictionary\n",
    "modeling_bundle = {\n",
    "    \"data\": {\n",
    "        \"portuguese\": {\n",
    "            \"train\": normalized_datasets[\"portuguese\"][\"train\"],\n",
    "            \"test\": normalized_datasets[\"portuguese\"][\"test\"]\n",
    "        },\n",
    "        \"math\": {\n",
    "            \"train\": normalized_datasets[\"math\"][\"train\"],\n",
    "            \"test\": normalized_datasets[\"math\"][\"test\"]\n",
    "        },\n",
    "        \"dropouts\": df_all_dropouts\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"features_base\": base_features,\n",
    "        \"features_sensitivity\": sensitivity_cols, # Tes indices : Parental_Edu_Level, Alcohol_Index, etc.\n",
    "        \"target\": \"G3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the bundle\n",
    "with open(export_filename, \"wb\") as f:\n",
    "    pickle.dump(modeling_bundle, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"--- Export Successful: {export_filename} ---\")\n",
    "print(f\"Metadata: 'features_base' and 'features_sensitivity' lists included.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 78523,
     "isSourceIdPinned": false,
     "sourceId": 180266,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.956093,
   "end_time": "2026-01-08T15:38:33.766641",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T15:38:22.810548",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
